{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Uncertainty\n",
    "\n",
    "- In probability, underlying processes are represented by a __model__ (often some distribution)   \n",
    "    e.g., flipping a coin, rolling a dice, etc.\n",
    "    \n",
    "- In reality, even for very simple processes, underlying process is not easily described   \n",
    "    e.g., what is the true $p$ of a coin? pmf of a dice?,\n",
    "    \n",
    "- Many real-world processes are much more complicated; however, we still try  \n",
    "    > _Now it would be very remarkable if any system existing in the real world could be \n",
    "    exactly represented by any simple model. However, cunningly chosen parsimonious models \n",
    "    often do provide remarkably useful approximations. \n",
    "    [... example of modeling gas in physics ...]  \n",
    "    For such a model there is no need to ask the question \"Is the model true?\". \n",
    "    If \"truth\" is to be the \"whole truth\" the answer must be \"No\". \n",
    "    The only question of interest is \"Is the model illuminating and useful?\"._  \n",
    "    George Box\n",
    "\n",
    "- If you build a model with random variables having known distributions, we can simulate the outcomes\n",
    "\n",
    "- If the model is accurate to some degree, we can say a lot about the outcome: i.e., the model is useful\n",
    "\n",
    "- However, often building models from ground up is practically impossible\n",
    "    e.g., where do you begin to build a model for NCAA tournament? human behavior?\n",
    "\n",
    "- Often we are interested in inference and/or prediction\n",
    "\n",
    "    - __Inference__: making conclusions based on data  \n",
    "        e.g., parameters such as winning probability for each team\n",
    "\n",
    "    - __Prediction__: making guesses about the next outcome  \n",
    "        e.g., who will win?\n",
    "        \n",
    "- Monte Carlo simulation is a good tool to illustrate the relationship between the generating process and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation\n",
    "\n",
    "First, recall the definition of a random sample:\n",
    "\n",
    "> __Random sample__: Let $X_1,X_2,\\dots,X_n$ be a set of $n$ independent random variables, \n",
    "all having the _same pdf_, $f_X$. Then $X_1,X_2,\\dots,X_n$ are said to be a _random sample of size n_.\n",
    "\n",
    "- Random sample: $X_1,X_2,\\dots,X_n$\n",
    "\n",
    "- Random measurements: $x_1,x_2,\\dots,x_n$\n",
    "\n",
    "While random sample is made up of random variables,\n",
    "__iid observations__ (independent identically distributed)\n",
    "$x_1, x_2,\\dots x_n$ are $n$-measurements (numbers) coming from $X_1,X_2,\\dots,X_n$\n",
    "\n",
    "- In order to capture randomness of _random sample_ with _iid observations_, \n",
    "    we can generate many sets of random measurements   \n",
    "    $$\n",
    "    x_1^{(1)},x_2^{(1)},\\dots,x_n^{(1)}\\\\\n",
    "    x_1^{(2)},x_2^{(2)},\\dots,x_n^{(2)}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_1^{(S)},x_2^{(S)},\\dots,x_n^{(S)}\n",
    "    $$\n",
    "\n",
    "- As an example, let's simulate convergence of the sample mean to the true mean\n",
    "\n",
    "### Consistency of the sample mean: dice example\n",
    "\n",
    "Suppose that $X_1,X_2,\\dots,X_n$ is a random sample of size $n$ from a discrete pdf $p_X(k;\\mu)$, where $E(X)=\\mu$ and $\\text{Var}(X) = \\sigma^2 < \\infty$. Let $\\hat\\mu_n = \\frac{1}{n}\\sum_i^n X_i$. Is $\\hat\\mu_n$ a consistent estimator for $\\mu$?\n",
    "\n",
    "Since we are simulating dice throw with a pmf, the _model_ is perfect. Hence, we know the true value of $E(X)=\\mu=3.5$ and $\\text{Var}(X) = \\sigma^2 = 2.917$. Also, sampling distribution says $E(\\hat\\mu_n)=\\mu$ and $$\\text{Var}(\\hat\\mu_n) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i)= \\frac{\\sigma^2}{n}$$. We will verify this empirically.\n",
    "\n",
    "First, generate many random observations $(x_1^{(s)},x_2^{(s)},\\dots ,x_n^{(s)})$ \n",
    "for $s=1,2,\\dots, 10$ and $n=7$. Each simulation is the _data_ of 7 dice throws.\n",
    "Put them into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n = 7\n",
    "S = 10\n",
    "\n",
    "x = np.random.randint(size=(S, n), low=1, high=7)\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"unique values in x:\", np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `x`, one set of observations is one row. Get one row by indexing into `x`. First index is for the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one row:\", x[0])    \n",
    "print(\"one value:\", x[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one row:\", x[-1])\n",
    "print(\"one value:\", x[-1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy offers many convenient and flexible ways to slice and get a subset of numpy arrays (Chapter 2 in Vanderplas). In particular, [_views_ of arrays](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html#Subarrays-as-no-copy-views) are useful. This is in contrast to R that always creates copies (even in function calls).\n",
    "\n",
    "Numpy makes array operations simple. For example, we can't use add a value to the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [1,2,3]+3  ## causes error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to common arithmetic functions (called ufuncs) other useful functions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply.outer(x[0],x[1]) ## outer product of first two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add.outer(x[:,0],x[:,1]) ## outer sum of first two columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sample mean of one set of random observations can be computed by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"object method:\", x[0].mean(), \"function:\", np.mean(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, they are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x[0]\n",
    "## x0.mean??\n",
    "## np.mean??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean of one row of `x` is sample mean of one simulation: i.e., mean of 50 dice throws. \n",
    "How do you compute mean of each simulation? Numpy `axis` refers to the direction of the array. \n",
    "\n",
    "Giving argument `axis=0`, the mean function computes column averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = x.mean(axis=0)\n",
    "print(\"x.mean(axis=0)   :\", temp)\n",
    "print(\"temp.mean(0)[3]  :\", temp[3])\n",
    "print(\"temp[:,3].mean() :\", x[:,3].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving argument `axis=1`, the mean function computes row averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = x.mean(axis=1)\n",
    "print(\"xmean:\", xmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `xmean` contains many observations of the sample mean $\\hat\\mu_n$. Plot the histogram of `xmean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # set plot style\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(11, 9)\n",
    "plt.hist(xmean)\n",
    "plt.xlabel('sample mean')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Sample Size $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chebyshev's inequality says that \n",
    "$$ \\Pr(|\\hat\\mu_n - \\mu| < \\epsilon) > 1 - \\frac{\\text{Var}(\\hat\\mu_n)}{\\epsilon^2} \n",
    "= 1- \\frac{\\sigma^2}{n\\epsilon^2} $$\n",
    "\n",
    "Recall that $\\sigma^2=2.917$ is the variance of outcomes of dice represented as a random variable. \n",
    "Fixing the error to $\\epsilon = 1$ makes the RHS a function of $n$.\n",
    "\n",
    "In order to empirically verify Chebyshev's inequality, we will simulate the dice with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## throw dice\n",
    "def throw_dice(n=1):\n",
    "    \n",
    "    from numpy.random import randint\n",
    "    \n",
    "    return(randint(size=n, low=1, high=7))\n",
    "\n",
    "## throw dice n times, compute mean\n",
    "def muhat_n(n=1):\n",
    "    \n",
    "    return(throw_dice(n).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above functions return numpy arrays. We will compile results below in a bare list. Then create Pandas data frame from it (more on Pandas later). Some automatic type conversion is possible, but more strict than R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_choices = np.arange(20, 10001, 10)\n",
    "\n",
    "results = []\n",
    "for n in n_choices:\n",
    "    results += [muhat_n(n)] ## appends to list\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'n_throws': n_choices,\n",
    "    'muhat_n' : results,\n",
    "})\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(11, 9)\n",
    "plt.plot(df['n_throws'], df['muhat_n']);\n",
    "plt.xlabel(\"Number of dice throws (n)\");\n",
    "plt.ylabel(\"Average of n dice throws\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build on the result by showing where 95% bound according to Chebyshev's inequality.\n",
    "\n",
    "If we choose $\\epsilon = 1$ and $n$ is fixed at 50. Then, plugging in $\\sigma^2$, we get:\n",
    "$$ \n",
    "1- \\frac{2.917}{n\\cdot \\epsilon^2} = 0.95\\\\\n",
    "\\epsilon = \\sqrt{\\frac{2.917}{0.05\\cdot n}}$$\n",
    "\n",
    "Following computes error threshold $epsilon$ guaranteed by Chebyshev's threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_n = np.sqrt(2.917/(0.05*n_choices))\n",
    "df['epsilon'] = epsilon_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the new plot\n",
    "$$ \\Pr(|\\hat\\mu_n - \\mu| < \\epsilon) > 0.95\\\\\n",
    "\\Pr(-\\epsilon_n < \\hat\\mu_n - \\mu < \\epsilon_n)  > 0.95 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(11, 9)\n",
    "plt.plot((df['n_throws']),  df['muhat_n']-3.5);    ## subtract true mean 3.5\n",
    "plt.plot((df['n_throws']),  df['epsilon'], '--r');\n",
    "plt.plot((df['n_throws']), -df['epsilon'], '--r');\n",
    "plt.xlabel(\"Number of dice throws (n)\");\n",
    "plt.ylabel(\"Average of n dice throws\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## repeat computing mean of n throws s times\n",
    "def repeat_muhat_n(n=1, s=100):\n",
    "    \n",
    "    out = [muhat_n(n) for one in range(s)]\n",
    "    \n",
    "    return({ 'min': min(out), 'max': max(out), 'mean': sum(out)/s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_muhat_n(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n in n_choices:\n",
    "    results += [repeat_muhat_n(n)] ## appends to list\n",
    "\n",
    "df_new = pd.DataFrame(results)\n",
    "df_new['n_throws'] = n_choices\n",
    "df_new['epsilon'] = epsilon_n\n",
    "df_new[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(11, 9)\n",
    "plt.plot((df_new['n_throws']),  df_new['mean']-3.5, '-g');    ## subtract true mean 3.5\n",
    "plt.plot((df_new['n_throws']),  df_new['min'] -3.5, ':g');    ## subtract true mean 3.5\n",
    "plt.plot((df_new['n_throws']),  df_new['max'] -3.5, ':g');    ## subtract true mean 3.5\n",
    "plt.plot((df_new['n_throws']),  df_new['epsilon'], '--r');\n",
    "plt.plot((df_new['n_throws']), -df_new['epsilon'], '--r');\n",
    "plt.xlabel(\"Number of dice throws (n)\");\n",
    "plt.ylabel(\"Average of n dice throws\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for 90% bound\n",
    "\n",
    "Plot $\\epsilon$ bounds corresponding to 80% and 90% probability. Interpret these bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_n = np.sqrt(2.917/(0.1*n_choices))\n",
    "df_new['epsilon_10'] = epsilon_n\n",
    "\n",
    "epsilon_n = np.sqrt(2.917/(0.2*n_choices))\n",
    "df_new['epsilon_20'] = epsilon_n\n",
    "\n",
    "figsize(11, 9)\n",
    "plt.plot((df_new['n_throws']),  df_new['mean']-3.5, '-g')    ## subtract true mean 3.5\n",
    "plt.plot((df_new['n_throws']),  df_new['min'] -3.5, ':g')    ## subtract true mean 3.5\n",
    "plt.plot((df_new['n_throws']),  df_new['max'] -3.5, ':g')    ## subtract true mean 3.5\n",
    "plt.plot((df_new['n_throws']),  df_new['epsilon'], '--r')\n",
    "plt.plot((df_new['n_throws']), -df_new['epsilon'], '--r')\n",
    "plt.plot((df_new['n_throws']),  df_new['epsilon_10'], '--b')\n",
    "plt.plot((df_new['n_throws']), -df_new['epsilon_10'], '--b')\n",
    "plt.plot((df_new['n_throws']),  df_new['epsilon_20'], '--k')\n",
    "plt.plot((df_new['n_throws']), -df_new['epsilon_20'], '--k')\n",
    "plt.xlim(0, 3000)\n",
    "plt.xlabel(\"Number of dice throws (n)\")\n",
    "plt.ylabel(\"Average of n dice throws\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from distribution vs data\n",
    "\n",
    "In simulation, we \"sample\" from the distribution: i.e., observe more measurements from the true underlying process. In the dice example, sampling from the distribution (true underlying process) is equivalent to the ability to execute the function `throw_dice()`. However, in real-world scenarios, we may not have that luxury: e.g., only be able to record some real dice 1000 times.\n",
    "\n",
    "In this section, we simulate such situation. There is some loaded dice with pmf as follows:\n",
    "\n",
    "|Outcome: $x$|1     |2     |3     |4     |5     |6     |\n",
    "|---------   |---   |---   |---   |---   |---   |---   |\n",
    "|$\\Pr(X=x)$  |1/12  |2/12  |2/12  |2/12  |2/12  |3/12  |\n",
    "\n",
    "Function for throwing such dice can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def throw_loaded_dice(n=1):\n",
    "    from numpy.random import choice\n",
    "    \n",
    "    d = [1, 2,2, 3,3, 4,4, 5,5, 6,6,6]\n",
    "    return(choice(d, n, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `d` represents is if one were to throw dice 12 times, one would get 1 exactly once, 2 exactly twice, 3 exactly twice, $\\dots$, 6 exactly three times. In terms of proportion (out of 12 times), you get exactly $\\Pr(X=x)$.\n",
    "\n",
    "Now we 'throw' the dice 10,000 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = throw_loaded_dice(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world situations, we may not have access to the real dice; however, we are given record of 100000 dice throws.\n",
    "\n",
    "In this case, we can _resample with replacement_ from the data. The data is our _empirical dice_ since we have 10,000 throws, each adding to count of one of the numbers 1 through 6.\n",
    "\n",
    "We can use the following function to sample from our _data_ instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_from_data(n=1, data_in=None):\n",
    "    from numpy.random import choice\n",
    "    \n",
    "    return(choice(data_in, n, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample from `data` 1000 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = choose_from_data(1000, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the distribution of the _true_ pmf, _empirical_ pmf, and _resampled_ pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome, data_counts = np.unique(data, return_counts=True)\n",
    "outcome, resample_counts = np.unique(pseudo_data, return_counts=True)\n",
    "\n",
    "pmfs = pd.DataFrame({\n",
    "    'outcome': np.arange(1, 7),\n",
    "    'true_pmf': np.array([1, 2, 2, 2, 2, 3])/12,\n",
    "    'empirical_pmf': data_counts/sum(data_counts),\n",
    "    'resample_pmf': resample_counts/sum(resample_counts),\n",
    "}, columns=['outcome','true_pmf','empirical_pmf','resample_pmf'])\n",
    "\n",
    "pmfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above experiment shows that empirical pmf and resample pmf are similar to the truth. This is the basis for using resampled data (such as bootstrap) for estimating sampling distributions, etc. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
